\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}
\usepackage[portrait, margin = 1in]{geometry}
\usepackage{graphicx}
\usepackage{scicite}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{amsmath}
\title{\texttt{BLUEs.jl}: Best Linear Unbiased Estimators for Julia}
\author{Brynnydd Hamilton, Jake Gebbie}
\date{\today}
\begin{document}
\maketitle

\section{\texttt{combine} function}

The master equations follow for the overdetermined formulation,
\begin{eqnarray}
  \label{eq:master5}
{\tilde {\bf x}} &=& ({\bf E}^T {\bf R}_{nn}^{-1} {\bf E} + {\bf C}^{-1}_{xx})^{-1} ({\bf E}^T {\bf R}_{nn}^{-1} {\bf y} + {\bf C}_{xx}^{-1} {\bf x}_0), \\
\label{eq:master6}
{\bf P} &=& ({\bf E}^T {\bf R}_{nn}^{-1} {\bf E} + {\bf C}_{xx}^{-1})^{-1},
\end{eqnarray}
and the underdetermined formulation,
\begin{eqnarray}
  \label{eq:master7}
{\tilde {\bf x}} &=& {\bf x}_0 + {\bf C}_{xx} {\bf E}^T ({\bf EC}_{xx} {\bf E}^T + {\bf R}_{nn})^{-1} ({\bf y}-{\bf Ex}_0) \\
\label{eq:master8}
\bf{P} &=& {\bf C}_{xx} - {\bf C}_{xx} {\bf E}^T ({\bf EC}_{xx} {\bf E}^T + {\bf R}_{nn})^{-1} {\bf EC}_{xx}.
\end{eqnarray}

\section{Single model ``textbook'' solution, no prior}
This math follows Section 1.3.4. in Dynamical Insights from Data, and derives the equation behind the ``solve\_textbook'' method in BLUEs.jl. 
We seek to solve the system
\begin{equation}
\label{eq:2}
\mathbf{y = Ex}
\end{equation}
where $\mathbf{y}$ is some vector of observations, $\mathbf{x}$ is some vector of parameters and $\mathbf{E}$ is a model matrix that relates the two. Our observations $\mathbf{y}$ has some associated noise covariance matrix $\mathbf{C_{nn}}$ Here we solve an overdetermined problem, so $\text{length}(\mathbf{y}) < \text{length}(\mathbf{x})$, and $\textbf{E}$ has more rows than columns. We define a residual $\mathbf{n}$
\begin{equation}
\label{eq:3}
\mathbf{n = y - Ex}
\end{equation}
We seek to minimize a cost function defined by the weighted inner product of the residual
\begin{equation}
\label{eq:1}
J = \mathbf{n^TC_{nn}^{-1}n = (y - Ex)^TC_{nn}^{-1}(y-Ex)}
\end{equation}
To minimize it, we want to minimize $J$ with respect to $\mathbf{x}$. To accomplish this, we take the first partial derivative $\frac{\partial J}{\partial \mathbf{x}}$, set it equal to 0, and solve for our solution estimate $\mathbf{\tilde{x}}$
\begin{equation}
\label{eq:4}
\frac{\partial J}{\partial\mathbf{x}} = \frac{\partial \mathbf{n}}{\partial \mathbf{x}}\frac{\partial J}{\partial \mathbf{n}}
\end{equation}
\begin{equation}
\label{eq:5}
\frac{\partial J}{\partial \mathbf{n}} = (\mathbf{C_{nn}}^{-1} + \mathbf{C_{nn}}^{-T})\mathbf{n} = 2 \mathbf{C_{nn}}^{-1} \mathbf{n}
\end{equation}
\begin{equation}
\label{eq:6}
\frac{\partial \mathbf{n}}{\partial \mathbf{x}} = -\mathbf{E}^{T}
\end{equation}
\begin{equation}
\label{eq:7}
\frac{\partial J}{\partial\mathbf{x}} = \frac{\partial \mathbf{n}}{\partial \mathbf{x}}\frac{\partial J}{\partial \mathbf{n}} = -2 \mathbf{E}^T \mathbf{C_{nn}}^{-1} \mathbf{n} = -2 \mathbf{E}^T \mathbf{C_{nn}}^{-1} (\mathbf{y} - \mathbf{Ex}) = -2 \mathbf{E}^T\mathbf{C_{nn}}^{-1}\mathbf{y} + 2 \mathbf{E}^T\mathbf{C_{nn}}^{-1}\mathbf{Ex}
\end{equation}
We then set the above equation equal to zero, and now solve for the solution estimate $\mathbf{\tilde{x}}$
\begin{equation}
  \label{eq:8}
 -2 \mathbf{E}^T\mathbf{C_{nn}}^{-1}\mathbf{y} + 2 \mathbf{E}^T\mathbf{C_{nn}}^{-1}\mathbf{E\tilde{x}}= 0 
\end{equation}
\begin{equation}
\label{eq:9}
\mathbf{\tilde{x}} = (\mathbf{E}^{T}\mathbf{C_{nn}}^{-1}\mathbf{E})^{-1}(\mathbf{E}^T\mathbf{C_{nn}}^{-1}\mathbf{y})
\end{equation}

\section{Multiple model ``textbook'' solution, no prior}
For our eventual application, we will have multiple, disparate observational vectors $\mathbf{y_{i}}$, that each have their own associated noise covariance matrix $\mathbf{C_{nn}}_{(i)}$, as well as their own model matrix $\mathbf{E}_i$. However, they will all be associated with the same parameter vector $\mathbf{x}$. Therefore, we are solving the simultaneous system
\begin{equation}
\label{eq:10}
\mathbf{y}_{i} = \mathbf{E}_{i}\mathbf{x}
\end{equation}
For each observational vector, we can define a corresponding noise vector
\begin{equation}
\label{eq:11}
\mathbf{n}_{i} = \mathbf{y_{i}} - \mathbf{E}_i\mathbf{x}
\end{equation}
Each system will have its own cost function, and we want to minimize all of them simultaneously, so we will minimize the sum of the cost functions
\begin{equation}
\label{eq:12}
J = \sum^{N}_{i} \mathbf{n}_{i}^T\mathbf{C_{nn}}_{(i)}\mathbf{n}_{i}
\end{equation}
As this is a simple sum, the math will remain relatively similar to Equations \ref{eq:4} - \ref{eq:7}, with the new version of Equation \ref{eq:7} as follows
\begin{equation}
\label{eq:13}
\frac{\partial J}{\partial \mathbf{x}} = \sum_{i}^{N} -2 \mathbf{E}_{i}^{T} \mathbf{C_{nn}}^{-1}_{(i)}\mathbf{y}_{i} + 2 \mathbf{E}^{T}_{i}\mathbf{C_{nn}}^{-1}_{(i)}\mathbf{E}_i\mathbf{x}
\end{equation}
If we set Equation \ref{eq:13} equal to 0, and solve for $\mathbf{\tilde{x}}$, we would have to calculate
\begin{equation}
\label{eq:14}
\mathbf{\tilde{x}} = \left(\sum_{i}^{N}\mathbf{E}^{T}_{i}\mathbf{C_{nn}}^{-1}_{(i)}\mathbf{E}_i\right)^{-1} \left(\sum_{i}^{N} \mathbf{E}_{i}^{T} \mathbf{C_{nn}}^{-1}_{(i)}\mathbf{y}_{i}\right)
\end{equation}
\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
